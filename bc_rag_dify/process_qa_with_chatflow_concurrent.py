import json
import requests
import time
import os
import re
import random
from pathlib import Path
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
from bert_score import score
import numpy as np
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
from tqdm import tqdm

# æ¸…ç†ä»£ç†ç¯å¢ƒå˜é‡ï¼Œé¿å…ä»£ç†è¿æ¥é—®é¢˜
for var in ("HTTP_PROXY", "HTTPS_PROXY", "ALL_PROXY", "http_proxy", "https_proxy", "all_proxy"):
    os.environ.pop(var, None)

# ChatFlowï¼ˆAIé—®ç­”ï¼‰é…ç½®
CHATFLOW_API_URL = 'https://api.dify.ai/v1/chat-messages'
CHATFLOW_API_KEY = 'Bearer app-W0Vf8phyDHlPRyDkat7HczOK'
CHATFLOW_HEADERS = {
    'Authorization': CHATFLOW_API_KEY,
    'Content-Type': 'application/json',
}

# å…¨å±€é”ï¼Œç”¨äºä¿æŠ¤æ¨¡å‹è®¿é—®
model_lock = threading.Lock()

# å…¨å±€æ¨¡å‹å˜é‡
embedding_model = None
nli_tokenizer = None
nli_model = None

def initialize_models():
    """åˆå§‹åŒ–è¯„ä¼°æ¨¡å‹"""
    global embedding_model, nli_tokenizer, nli_model
    
    print("ğŸ”§ æ­£åœ¨åŠ è½½è¯„ä¼°æ¨¡å‹...")
    
    # 1. æ–‡æœ¬åµŒå…¥æ¨¡å‹ (ç”¨äºContext Relevance) - å¿…éœ€
    try:
        embedding_model = SentenceTransformer('shibing624/text2vec-base-chinese')
        print("âœ… æ–‡æœ¬åµŒå…¥æ¨¡å‹åŠ è½½å®Œæˆ")
    except Exception as e:
        print(f"âŒ æ–‡æœ¬åµŒå…¥æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        exit(1)
    
    # 2. NLIæ¨¡å‹ (ç”¨äºAnswer Faithfulnesså’ŒAnswer Relevance) - å¯é€‰
    try:
        nli_model_name = 'MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli'
        nli_tokenizer = AutoTokenizer.from_pretrained(nli_model_name)
        nli_model = AutoModelForSequenceClassification.from_pretrained(nli_model_name)
        nli_model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        print("âœ… NLIæ¨¡å‹åŠ è½½å®Œæˆ")
    except Exception as e:
        print(f"âš ï¸ NLIæ¨¡å‹åŠ è½½å¤±è´¥ï¼Œå°†ä½¿ç”¨Embeddingå›é€€: {e}")
        nli_tokenizer = None
        nli_model = None
    
    # 3. BERTScoreæ¨¡å‹ (ç”¨äºAnswer Correctness)
    print("âœ… BERTScoreæ¨¡å‹å‡†å¤‡å°±ç»ª")

def call_chatflow_api_with_retry(question, max_retries=3):
    """è°ƒç”¨ChatFlow APIï¼Œå¸¦é‡è¯•æœºåˆ¶"""
    data = {
        "inputs": {},
        "query": question,
        "response_mode": "blocking",
        "user": "test_user_001"
    }
    
    for attempt in range(max_retries):
        try:
            # æ·»åŠ éšæœºå»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
            time.sleep(random.uniform(1, 3))
            
            response = requests.post(
                CHATFLOW_API_URL, 
                headers=CHATFLOW_HEADERS, 
                data=json.dumps(data), 
                timeout=60,  # å‡å°‘è¶…æ—¶æ—¶é—´
                verify=False  # ç¦ç”¨SSLéªŒè¯
            )
            
            if response.status_code == 200:
                return response.json()
            elif response.status_code == 429:  # Rate limit
                wait_time = (attempt + 1) * 5
                print(f"  â³ é‡åˆ°é¢‘ç‡é™åˆ¶ï¼Œç­‰å¾… {wait_time} ç§’...")
                time.sleep(wait_time)
                continue
            else:
                print(f"  âš ï¸ APIè°ƒç”¨å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                if attempt < max_retries - 1:
                    time.sleep(2)
                    continue
                return None
                
        except requests.exceptions.RequestException as e:
            print(f"  âŒ APIè°ƒç”¨å¼‚å¸¸ (å°è¯• {attempt + 1}/{max_retries}): {e}")
            if attempt < max_retries - 1:
                time.sleep(2)
                continue
            return None
    
    return None

def extract_context_and_answer(response_data):
    """ä»APIå“åº”ä¸­æå–retrieved_contextå’Œanswer"""
    try:
        # è·å–answerå­—æ®µ
        answer_text = response_data.get('answer', '')
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«<think>æ ‡ç­¾ï¼Œå¦‚æœæœ‰åˆ™ç§»é™¤
        if '<think>' in answer_text and '</think>' in answer_text:
            # ç§»é™¤<think>éƒ¨åˆ†ï¼Œåªä¿ç•™æœ€ç»ˆç­”æ¡ˆ
            think_start = answer_text.find('<think>')
            think_end = answer_text.find('</think>') + len('</think>')
            answer_text = answer_text[think_end:].strip()
        
        # ç°åœ¨å¤„ç†æ¸…ç†åçš„answer_text
        # æ£€æŸ¥æ˜¯å¦åŒ…å«"ç¬¬ä¸€éƒ¨åˆ†ï¼š**retrieved_context**"å’Œ"ç¬¬äºŒéƒ¨åˆ†ï¼š**answer**"çš„æ ¼å¼
        if "ç¬¬ä¸€éƒ¨åˆ†ï¼š**retrieved_context**" in answer_text and "ç¬¬äºŒéƒ¨åˆ†ï¼š**answer**" in answer_text:
            # åˆ†å‰²retrieved_contextå’Œansweréƒ¨åˆ†
            parts = answer_text.split("ç¬¬äºŒéƒ¨åˆ†ï¼š**answer**")
            if len(parts) == 2:
                # æå–retrieved_contextéƒ¨åˆ†
                context_part = parts[0]
                context_start = context_part.find("ç¬¬ä¸€éƒ¨åˆ†ï¼š**retrieved_context**")
                if context_start != -1:
                    # ç§»é™¤æ ‡é¢˜ï¼Œè·å–å†…å®¹
                    context_content = context_part[context_start + len("ç¬¬ä¸€éƒ¨åˆ†ï¼š**retrieved_context**"):].strip()
                    # ç§»é™¤å¯èƒ½çš„æ ¼å¼æ ‡è®°
                    context_content = re.sub(r'^[-\s]*', '', context_content)
                    context_content = re.sub(r'[-\s]*$', '', context_content)
                    # æ£€æŸ¥æ˜¯å¦ä¸ºç©ºæˆ–åªåŒ…å«[]
                    if context_content.strip() in ['', '[]', 'ã€ã€‘', '()', 'ï¼ˆï¼‰']:
                        retrieved_context = ''
                    else:
                        retrieved_context = context_content
                else:
                    retrieved_context = ''
                
                # æå–answeréƒ¨åˆ†
                answer_part = parts[1].strip()
                # ç§»é™¤å¯èƒ½çš„æ ¼å¼æ ‡è®°å’Œæ³¨é‡Š
                answer_part = re.sub(r'ï¼ˆæ³¨ï¼š.*?ï¼‰', '', answer_part, flags=re.DOTALL)
                answer_part = re.sub(r'æ›´æ­£åçš„ç¬¬äºŒéƒ¨åˆ†ï¼š', '', answer_part)
                answer_part = re.sub(r'^\s*[-â€”]\s*', '', answer_part)
                answer_part = answer_part.strip()
                
                return retrieved_context, answer_part
            else:
                return '', answer_text
        else:
            # å¦‚æœæ²¡æœ‰æ˜ç¡®çš„æ ¼å¼ï¼Œå°è¯•å…¶ä»–è§£ææ–¹å¼
            # å°è¯•è§£æJSONæ ¼å¼çš„answer
            try:
                # æŸ¥æ‰¾JSONå¼€å§‹å’Œç»“æŸçš„ä½ç½®
                start_idx = answer_text.find('{')
                end_idx = answer_text.rfind('}') + 1
                
                if start_idx != -1 and end_idx > start_idx:
                    json_str = answer_text[start_idx:end_idx]
                    parsed = json.loads(json_str)
                    
                    retrieved_context = parsed.get('retrieved_context', '')
                    ai_answer = parsed.get('answer', '')
                    
                    return retrieved_context, ai_answer
                else:
                    return '', answer_text
                    
            except json.JSONDecodeError as e:
                return '', answer_text
                
    except Exception as e:
        return '', ''

def calculate_context_relevance(question, retrieved_context):
    """è®¡ç®—ä¸Šä¸‹æ–‡ç›¸å…³æ€§ (Context Relevance)"""
    try:
        with model_lock:
            if embedding_model is None:
                print("âŒ æ¨¡å‹æœªåˆå§‹åŒ–")
                return 0.0
            embeddings = embedding_model.encode([question, retrieved_context])
            similarity = np.dot(embeddings[0], embeddings[1]) / (np.linalg.norm(embeddings[0]) * np.linalg.norm(embeddings[1]))
            return float(similarity)
    except Exception as e:
        print(f"  âŒ Context Relevanceè®¡ç®—å¤±è´¥: {e}")
        return 0.0

def calculate_answer_faithfulness(retrieved_context, ai_answer):
    """è®¡ç®—ç­”æ¡ˆå¿ å®åº¦ (Answer Faithfulness) - ä¼˜å…ˆä½¿ç”¨NLIæ¨¡å‹ï¼Œå¤±è´¥æ—¶å›é€€åˆ°Embedding"""
    try:
        # é¦–å…ˆå°è¯•ä½¿ç”¨NLIæ¨¡å‹
        if nli_tokenizer is not None and nli_model is not None:
            with model_lock:
                # å‡†å¤‡è¾“å…¥
                inputs = nli_tokenizer(
                    retrieved_context, 
                    ai_answer, 
                    return_tensors="pt", 
                    truncation=True, 
                    max_length=512
                )
                
                with torch.no_grad():
                    outputs = nli_model(**inputs)
                    logits = outputs.logits
                    probs = torch.softmax(logits, dim=1)
                    
                    # è·å–entailmentæ¦‚ç‡ (é€šå¸¸ç´¢å¼•ä¸º2)
                    entailment_prob = probs[0][2].item()
                    return float(entailment_prob)
        else:
            # NLIæ¨¡å‹ä¸å¯ç”¨ï¼Œä½¿ç”¨embeddingä½™å¼¦ç›¸ä¼¼åº¦
            with model_lock:
                if embedding_model is None:
                    print("âŒ æ¨¡å‹æœªåˆå§‹åŒ–")
                    return 0.0
                embeddings = embedding_model.encode([retrieved_context, ai_answer])
                similarity = np.dot(embeddings[0], embeddings[1]) / (np.linalg.norm(embeddings[0]) * np.linalg.norm(embeddings[1]))
                return float(similarity)
    except Exception as e:
        print(f"  âš ï¸ NLIæ¨¡å‹è®¡ç®—å¤±è´¥ï¼Œä½¿ç”¨Embeddingå›é€€: {e}")
        # å›é€€åˆ°embeddingæ–¹æ³•
        try:
            with model_lock:
                if embedding_model is None:
                    print("âŒ æ¨¡å‹æœªåˆå§‹åŒ–")
                    return 0.0
                embeddings = embedding_model.encode([retrieved_context, ai_answer])
                similarity = np.dot(embeddings[0], embeddings[1]) / (np.linalg.norm(embeddings[0]) * np.linalg.norm(embeddings[1]))
                return float(similarity)
        except Exception as e2:
            print(f"  âŒ Answer Faithfulnessè®¡ç®—å¤±è´¥: {e2}")
            return 0.0

def calculate_answer_relevance(question, ai_answer):
    """è®¡ç®—ç­”æ¡ˆç›¸å…³æ€§ (Answer Relevance) - ä¼˜å…ˆä½¿ç”¨NLIæ¨¡å‹ï¼Œå¤±è´¥æ—¶å›é€€åˆ°Embedding"""
    try:
        # é¦–å…ˆå°è¯•ä½¿ç”¨NLIæ¨¡å‹
        if nli_tokenizer is not None and nli_model is not None:
            with model_lock:
                # å‡†å¤‡è¾“å…¥
                inputs = nli_tokenizer(
                    question, 
                    ai_answer, 
                    return_tensors="pt", 
                    truncation=True, 
                    max_length=512
                )
                
                with torch.no_grad():
                    outputs = nli_model(**inputs)
                    logits = outputs.logits
                    probs = torch.softmax(logits, dim=1)
                    
                    # è·å–entailmentæ¦‚ç‡ (é€šå¸¸ç´¢å¼•ä¸º2)
                    entailment_prob = probs[0][2].item()
                    return float(entailment_prob)
        else:
            # NLIæ¨¡å‹ä¸å¯ç”¨ï¼Œä½¿ç”¨embeddingä½™å¼¦ç›¸ä¼¼åº¦
            with model_lock:
                if embedding_model is None:
                    print("âŒ æ¨¡å‹æœªåˆå§‹åŒ–")
                    return 0.0
                embeddings = embedding_model.encode([question, ai_answer])
                similarity = np.dot(embeddings[0], embeddings[1]) / (np.linalg.norm(embeddings[0]) * np.linalg.norm(embeddings[1]))
                return float(similarity)
    except Exception as e:
        print(f"  âš ï¸ NLIæ¨¡å‹è®¡ç®—å¤±è´¥ï¼Œä½¿ç”¨Embeddingå›é€€: {e}")
        # å›é€€åˆ°embeddingæ–¹æ³•
        try:
            with model_lock:
                if embedding_model is None:
                    print("âŒ æ¨¡å‹æœªåˆå§‹åŒ–")
                    return 0.0
                embeddings = embedding_model.encode([question, ai_answer])
                similarity = np.dot(embeddings[0], embeddings[1]) / (np.linalg.norm(embeddings[0]) * np.linalg.norm(embeddings[1]))
                return float(similarity)
        except Exception as e2:
            print(f"  âŒ Answer Relevanceè®¡ç®—å¤±è´¥: {e2}")
            return 0.0

def calculate_answer_correctness(ai_answer, ref_answer):
    """è®¡ç®—ç­”æ¡ˆæ­£ç¡®æ€§ (Answer Correctness) - ä¼˜å…ˆä½¿ç”¨BERTScoreï¼Œå¤±è´¥æ—¶å›é€€åˆ°Embedding"""
    try:
        # ä½¿ç”¨BERTScoreè®¡ç®—è¯­ä¹‰åŒ¹é…åº¦
        P, R, F1 = score([ai_answer], [ref_answer], lang='zh', verbose=False)
        return float(F1[0])
    except Exception as e:
        print(f"  âš ï¸ BERTScoreè®¡ç®—å¤±è´¥ï¼Œä½¿ç”¨Embeddingå›é€€: {e}")
        # å›é€€åˆ°embeddingæ–¹æ³•
        try:
            with model_lock:
                if embedding_model is None:
                    print("âŒ æ¨¡å‹æœªåˆå§‹åŒ–")
                    return 0.0
                embeddings = embedding_model.encode([ai_answer, ref_answer])
                similarity = np.dot(embeddings[0], embeddings[1]) / (np.linalg.norm(embeddings[0]) * np.linalg.norm(embeddings[1]))
                return float(similarity)
        except Exception as e2:
            print(f"  âŒ Answer Correctnessè®¡ç®—å¤±è´¥: {e2}")
            return 0.0

def extract_keywords(text):
    """æå–å…³é”®è¯/å®ä½“"""
    # ä¸°å¯Œçš„ä¸­è‹±æ–‡åŒ»ç–—å…³é”®è¯
    medical_keywords = [
        # ä¸­æ–‡å…³é”®è¯
        'ä¹³å¤´', 'æº¢æ¶²', 'ä¹³æˆ¿', 'ä¹³è…º', 'æœˆç»', 'æ£€æŸ¥', 'æ²»ç–—', 'åŒ»é™¢', 'åŒ»ç”Ÿ',
        'ç—‡çŠ¶', 'ç–¼ç—›', 'èƒ€ç—›', 'ç™½è‰²', 'æ¶²ä½“', 'æ¿€ç´ ', 'é›Œæ¿€ç´ ', 'å‚¬ä¹³ç´ ',
        'å¢ç”Ÿ', 'å¯¼ç®¡', 'ç»†èƒ', 'ç—…ç†', 'è¶…å£°', 'é’¼é¶', 'ä¹³é€', 'è‚¿å—',
        'ç»“èŠ‚', 'å›Šè‚¿', 'çº¤ç»´ç˜¤', 'ä¹³è…ºç™Œ', 'è‰¯æ€§', 'æ¶æ€§', 'è½¬ç§»',
        'æ‰‹æœ¯', 'åŒ–ç–—', 'æ”¾ç–—', 'é¶å‘æ²»ç–—', 'å…ç–«æ²»ç–—', 'å†…åˆ†æ³Œæ²»ç–—',
        'æ´»æ£€', 'ç©¿åˆº', 'åˆ‡ç‰‡', 'å…ç–«ç»„åŒ–', 'åŸºå› æ£€æµ‹', 'åˆ†å­åˆ†å‹',
        'åˆ†æœŸ', 'åˆ†çº§', 'é¢„å', 'å¤å‘', 'ç”Ÿå­˜ç‡', 'æ²»æ„ˆç‡',
        'ç­›æŸ¥', 'é¢„é˜²', 'æ—©æœŸå‘ç°', 'å®šæœŸæ£€æŸ¥', 'è‡ªæ£€', 'è§¦è¯Š',
        'å½±åƒå­¦', 'CT', 'MRI', 'PET-CT', 'éª¨æ‰«æ', 'æ·‹å·´ç»“',
        'è…‹çª', 'é”éª¨', 'èƒ¸éª¨', 'è‚‹éª¨', 'è‚º', 'è‚', 'è„‘', 'éª¨',
        
        # è‹±æ–‡å…³é”®è¯
        'breast', 'nipple', 'discharge', 'lump', 'mass', 'tumor', 'cancer',
        'carcinoma', 'adenocarcinoma', 'ductal', 'lobular', 'invasive',
        'in situ', 'DCIS', 'LCIS', 'IDC', 'ILC', 'HER2', 'ER', 'PR',
        'triple negative', 'hormone receptor', 'estrogen', 'progesterone',
        'prolactin', 'mammogram', 'ultrasound', 'biopsy', 'fine needle',
        'core needle', 'excisional', 'incisional', 'sentinel node',
        'axillary', 'lymph node', 'metastasis', 'metastatic', 'stage',
        'grade', 'prognosis', 'survival', 'recurrence', 'remission',
        'chemotherapy', 'radiation', 'surgery', 'mastectomy', 'lumpectomy',
        'reconstruction', 'implant', 'flap', 'radiation therapy',
        'targeted therapy', 'immunotherapy', 'hormone therapy',
        'tamoxifen', 'aromatase inhibitor', 'herceptin', 'perjeta',
        'kadcyla', 'enhertu', 'ibrance', 'verzenio', 'kisqali',
        'screening', 'prevention', 'early detection', 'risk factors',
        'family history', 'genetic', 'BRCA1', 'BRCA2', 'PALB2',
        'TP53', 'PTEN', 'ATM', 'CHEK2', 'genetic testing',
        'counseling', 'surveillance', 'prophylactic', 'mastectomy',
        'oophorectomy', 'salpingo-oophorectomy', 'hysterectomy',
        
        # ç—‡çŠ¶ç›¸å…³
        'pain', 'tenderness', 'swelling', 'redness', 'warmth',
        'itching', 'rash', 'dimpling', 'puckering', 'retraction',
        'inversion', 'discharge', 'bleeding', 'ulceration', 'fever',
        'fatigue', 'weight loss', 'appetite', 'nausea', 'vomiting',
        'constipation', 'diarrhea', 'cough', 'shortness of breath',
        'chest pain', 'back pain', 'bone pain', 'headache',
        'dizziness', 'confusion', 'seizure', 'paralysis',
        
        # æ£€æŸ¥æ–¹æ³•
        'physical exam', 'clinical breast exam', 'mammography',
        'digital mammogram', '3D mammogram', 'tomosynthesis',
        'breast MRI', 'breast ultrasound', 'elastography',
        'contrast enhanced', 'diffusion weighted', 'spectroscopy',
        'nuclear medicine', 'scintigraphy', 'bone scan',
        'liver scan', 'brain scan', 'PET scan', 'CT scan',
        'chest X-ray', 'abdominal CT', 'pelvic CT', 'brain MRI',
        
        # æ²»ç–—ç›¸å…³
        'surgery', 'lumpectomy', 'mastectomy', 'breast conserving',
        'modified radical', 'radical', 'simple', 'skin sparing',
        'nipple sparing', 'reconstruction', 'implant', 'tissue expander',
        'autologous', 'TRAM flap', 'DIEP flap', 'latissimus dorsi',
        'chemotherapy', 'neoadjuvant', 'adjuvant', 'palliative',
        'radiation', 'external beam', 'brachytherapy', 'intraoperative',
        'hormone therapy', 'endocrine therapy', 'targeted therapy',
        'immunotherapy', 'clinical trial', 'protocol', 'regimen',
        
        # è¯ç‰©ç›¸å…³
        'doxorubicin', 'adriamycin', 'cyclophosphamide', 'cytoxan',
        'methotrexate', '5-fluorouracil', '5-FU', 'paclitaxel',
        'taxol', 'docetaxel', 'taxotere', 'carboplatin', 'cisplatin',
        'capecitabine', 'xeloda', 'gemcitabine', 'gemzar',
        'vinorelbine', 'navelbine', 'eribulin', 'halaven',
        'tamoxifen', 'nolvadex', 'raloxifene', 'evista',
        'anastrozole', 'arimidex', 'letrozole', 'femara',
        'exemestane', 'aromasin', 'fulvestrant', 'faslodex',
        'trastuzumab', 'herceptin', 'pertuzumab', 'perjeta',
        'ado-trastuzumab', 'kadcyla', 'fam-trastuzumab', 'enhertu',
        'palbociclib', 'ibrance', 'ribociclib', 'kisqali',
        'abemaciclib', 'verzenio', 'alpelisib', 'piqray',
        'olaparib', 'lynparza', 'talazoparib', 'talzenna'
    ]
    
    found_keywords = []
    for keyword in medical_keywords:
        if keyword.lower() in text.lower():  # ä¸åŒºåˆ†å¤§å°å†™
            found_keywords.append(keyword)
    
    return found_keywords

def calculate_context_recall(ref_answer, retrieved_context):
    """è®¡ç®—ä¸Šä¸‹æ–‡å¬å›ç‡ (Context Recall)"""
    try:
        # å¦‚æœretrieved_contextä¸ºç©ºï¼Œç›´æ¥è¿”å›0
        if not retrieved_context or retrieved_context.strip() == '':
            return 0.0
        
        # æå–ref_answerä¸­çš„å…³é”®è¯
        ref_keywords = extract_keywords(ref_answer)
        
        if not ref_keywords:
            return 0.0
        
        # ç»Ÿè®¡è¿™äº›å…³é”®è¯åœ¨retrieved_contextä¸­å‡ºç°çš„æ¯”ä¾‹ï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰
        found_count = 0
        retrieved_context_lower = retrieved_context.lower()
        
        for keyword in ref_keywords:
            if keyword.lower() in retrieved_context_lower:
                found_count += 1
        
        recall = found_count / len(ref_keywords)
        return recall
    except Exception as e:
        print(f"  âŒ Context Recallè®¡ç®—å¤±è´¥: {e}")
        return 0.0

def calculate_rag_score_mean(scores):
    """è®¡ç®—RAGè¯„åˆ†å‡å€¼"""
    valid_scores = [score for score in scores.values() if score is not None]
    if valid_scores:
        return sum(valid_scores) / len(valid_scores)
    return 0.0

def process_single_qa_file(qa_file_path):
    """å¤„ç†å•ä¸ªQAæ–‡ä»¶å¹¶è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
    try:
        # è¯»å–QAæ–‡ä»¶
        with open(qa_file_path, 'r', encoding='utf-8') as f:
            qa_data = json.load(f)
        
        question = qa_data['question']
        ref_answer = qa_data['answers'][0] if qa_data['answers'] else ''
        
        # è°ƒç”¨ChatFlow APIï¼ˆå¸¦é‡è¯•ï¼‰
        response = call_chatflow_api_with_retry(question)
        
        if response:
            retrieved_context, ai_answer = extract_context_and_answer(response)
            
            # è®¡ç®—å„é¡¹è¯„ä¼°æŒ‡æ ‡
            context_relevance = calculate_context_relevance(question, retrieved_context)
            faithfulness = calculate_answer_faithfulness(retrieved_context, ai_answer)
            answer_relevance = calculate_answer_relevance(question, ai_answer)
            answer_correctness = calculate_answer_correctness(ai_answer, ref_answer)
            context_recall = calculate_context_recall(ref_answer, retrieved_context)
            
            # è®¡ç®—RAGè¯„åˆ†å‡å€¼
            scores = {
                'context_relevance': context_relevance,
                'faithfulness': faithfulness,
                'answer_relevance': answer_relevance,
                'answer_correctness': answer_correctness,
                'context_recall': context_recall
            }
            rag_score_mean = calculate_rag_score_mean(scores)
            
            # åˆ›å»ºç»“æœæ•°æ®
            result_data = {
                "question": question,
                "retrieved_context": retrieved_context,
                "ai_answer": ai_answer,
                "ref_answer": ref_answer,
                "context_relevance": context_relevance,
                "faithfulness": faithfulness,
                "answer_relevance": answer_relevance,
                "answer_correctness": answer_correctness,
                "context_recall": context_recall,
                "rag_score_mean": rag_score_mean
            }
            
            return {
                'file_name': qa_file_path.name,
                'success': True,
                'result_data': result_data,
                'scores': scores
            }
        else:
            return {
                'file_name': qa_file_path.name,
                'success': False,
                'error': 'APIè°ƒç”¨å¤±è´¥'
            }
            
    except Exception as e:
        return {
            'file_name': qa_file_path.name,
            'success': False,
            'error': str(e)
        }

def process_all_qa_files():
    """å¹¶å‘å¤„ç†æ‰€æœ‰QAæ–‡ä»¶"""
    # åˆå§‹åŒ–æ¨¡å‹
    initialize_models()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path('chatflow_results')
    output_dir.mkdir(exist_ok=True)
    
    # è·å–å‰20ä¸ªqaæ–‡ä»¶ï¼ˆç¬¬1-10å’Œ100-109ï¼‰
    qa_dir = Path('breast_cancer_cmed_files')
    all_files = sorted([f for f in qa_dir.glob('qa*.json')])
    
    # é€‰æ‹©ç¬¬1-10å’Œ100-109è¿™20ä¸ªæ–‡ä»¶
    selected_files = []
    for i in range(1, 11):  # 1-10
        filename = f'qa{i:02d}_cmed.json'
        file_path = qa_dir / filename
        if file_path.exists():
            selected_files.append(file_path)
    
    for i in range(100, 110):  # 100-109
        filename = f'qa{i:03d}_cmed.json'
        file_path = qa_dir / filename
        if file_path.exists():
            selected_files.append(file_path)
    
    qa_files = selected_files
    
    print(f"ğŸš€ å¼€å§‹å¹¶å‘å¤„ç† {len(qa_files)} ä¸ªQAæ–‡ä»¶...")
    print(f"ğŸ”§ ä½¿ç”¨5ä¸ªå¹¶å‘çº¿ç¨‹")
    
    # å­˜å‚¨æ‰€æœ‰ç»“æœ
    all_results = []
    all_scores = []
    
    # ä½¿ç”¨ThreadPoolExecutorè¿›è¡Œå¹¶å‘å¤„ç†
    with ThreadPoolExecutor(max_workers=5) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        future_to_file = {executor.submit(process_single_qa_file, qa_file): qa_file for qa_file in qa_files}
        
        # ä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦
        with tqdm(total=len(qa_files), desc="å¤„ç†è¿›åº¦") as pbar:
            for future in as_completed(future_to_file):
                result = future.result()
                all_results.append(result)
                
                if result['success']:
                    # ä¿å­˜ç»“æœæ–‡ä»¶
                    result_filename = f"result{result['file_name'].replace('qa', '').replace('.json', '')}_cmed.json"
                    result_filepath = output_dir / result_filename
                    
                    with open(result_filepath, 'w', encoding='utf-8') as f:
                        json.dump(result['result_data'], f, ensure_ascii=False, indent=2)
                    
                    all_scores.append(result['scores'])
                    print(f"âœ… {result['file_name']} - RAG Score: {result['result_data']['rag_score_mean']:.4f}")
                else:
                    print(f"âŒ {result['file_name']} - {result['error']}")
                
                pbar.update(1)
    
    # è®¡ç®—å¹³å‡åˆ†
    if all_scores:
        avg_scores = {}
        for metric in ['context_relevance', 'faithfulness', 'answer_relevance', 'answer_correctness', 'context_recall']:
            values = [score[metric] for score in all_scores if score[metric] is not None]
            avg_scores[metric] = sum(values) / len(values) if values else 0.0
        
        # è®¡ç®—æ€»å¹³å‡åˆ†
        total_avg = sum(avg_scores.values()) / len(avg_scores)
        
        print(f"\nğŸ“Š å¤„ç†å®Œæˆï¼å…±å¤„ç† {len(all_scores)} ä¸ªæ–‡ä»¶")
        print(f"ğŸ“ˆ å„é¡¹æŒ‡æ ‡å¹³å‡åˆ†:")
        print(f"   - Context Relevance: {avg_scores['context_relevance']:.4f}")
        print(f"   - Faithfulness: {avg_scores['faithfulness']:.4f}")
        print(f"   - Answer Relevance: {avg_scores['answer_relevance']:.4f}")
        print(f"   - Answer Correctness: {avg_scores['answer_correctness']:.4f}")
        print(f"   - Context Recall: {avg_scores['context_recall']:.4f}")
        print(f"ğŸ“Š æ€»å¹³å‡åˆ†: {total_avg:.4f}")
        
        # ä¿å­˜æ±‡æ€»ç»“æœ
        summary_data = {
            "total_files_processed": len(all_scores),
            "average_scores": avg_scores,
            "total_average": total_avg,
            "successful_files": [r['file_name'] for r in all_results if r['success']],
            "failed_files": [r['file_name'] for r in all_results if not r['success']]
        }
        
        summary_filepath = output_dir / "summary_results.json"
        with open(summary_filepath, 'w', encoding='utf-8') as f:
            json.dump(summary_data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“„ æ±‡æ€»ç»“æœå·²ä¿å­˜åˆ°: summary_results.json")
    else:
        print("âŒ æ²¡æœ‰æˆåŠŸå¤„ç†çš„æ–‡ä»¶")

if __name__ == "__main__":
    process_all_qa_files() 