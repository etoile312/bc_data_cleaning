"""
ç»“æ„åŒ–æ•°æ®ç”Ÿæˆä¸»å…¥å£è„šæœ¬ã€‚
æ•´åˆå­—æ®µè§£æã€é‡‡æ ·ã€ç»„åˆç­‰æ¨¡å—ï¼Œç”Ÿæˆå®Œæ•´çš„ç»“æ„åŒ–ç—…ä¾‹æ•°æ®ã€‚
"""

import json
import os
from typing import Dict, List, Any

from .field_parser import FieldRuleParser
from .field_sampler import FieldSampler
from .combine_segments import SegmentCombiner
from .segment_groups import SEGMENT_GROUPS

class StructuredDataGenerator:
    """ç»“æ„åŒ–æ•°æ®ç”Ÿæˆå™¨"""
    
    def __init__(self, 
                 rules_path: str = 'rules/breast_cancer_rules.json',
                 segment_dir: str = 'data/segments',
                 output_dir: str = 'data'):
        """
        åˆå§‹åŒ–ç”Ÿæˆå™¨
        
        Args:
            rules_path: è§„åˆ™æ–‡ä»¶è·¯å¾„
            segment_dir: æ®µè½æ•°æ®ç›®å½•
            output_dir: è¾“å‡ºç›®å½•
        """
        self.rules_path = rules_path
        self.segment_dir = segment_dir
        self.output_dir = output_dir
        
        # åˆå§‹åŒ–å„ä¸ªæ¨¡å—
        self.parser = FieldRuleParser(rules_path)
        self.sampler = FieldSampler(
            self.parser.get_field_defs(),
            self.parser.get_logic_rules()
        )
        self.combiner = SegmentCombiner(segment_dir, rules_path)
    
    def generate_segment_pools(self, samples_per_group: int = 100) -> Dict[str, List[Dict[str, str]]]:
        """ç”Ÿæˆå„å­—æ®µç»„çš„æ ·æœ¬æ± """
        pools = {}
        
        for group_name, fields in SEGMENT_GROUPS.items():
            print(f"ç”Ÿæˆ {group_name} å­—æ®µç»„æ ·æœ¬æ± ...")
            samples = self.sampler.sample_segment(fields, samples_per_group, group_name)
            pools[group_name] = samples
            
            # ä¿å­˜åˆ°æ–‡ä»¶
            output_file = os.path.join(self.segment_dir, f'{group_name}_pool.json')
            os.makedirs(os.path.dirname(output_file), exist_ok=True)
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(samples, f, ensure_ascii=False, indent=2)
            print(f"âœ… å·²ä¿å­˜ {len(samples)} ä¸ª {group_name} æ ·æœ¬åˆ° {output_file}")
        
        return pools
    
    def generate_combined_cases(self, n_cases: int = 100) -> List[Dict[str, Any]]:
        """ç”Ÿæˆç»„åˆç—…ä¾‹"""
        print(f"ç”Ÿæˆ {n_cases} ä¸ªç»„åˆç—…ä¾‹...")
        cases = self.combiner.generate_cases(n_cases, os.path.join(self.output_dir, 'combined_cases.json'))
        return cases
    
    def validate_generated_data(self, cases: List[Dict[str, Any]]) -> Dict[str, Any]:
        """éªŒè¯ç”Ÿæˆçš„æ•°æ®è´¨é‡"""
        validation_results = {
            'total_cases': len(cases),
            'field_coverage': {},
            'logic_violations': [],
            'empty_fields': {}
        }
        
        # ç»Ÿè®¡å­—æ®µè¦†ç›–ç‡
        all_fields = self.parser.get_all_fields()
        field_counts = {field: 0 for field in all_fields}
        
        for case in cases:
            for field in all_fields:
                if field in case and case[field]:
                    field_counts[field] += 1
        
        validation_results['field_coverage'] = {
            field: count / len(cases) 
            for field, count in field_counts.items()
        }
        
        # æ£€æŸ¥ç©ºå­—æ®µ
        for field, coverage in validation_results['field_coverage'].items():
            if coverage < 0.1:  # è¦†ç›–ç‡ä½äº10%
                validation_results['empty_fields'][field] = coverage
        
        print(f"âœ… æ•°æ®éªŒè¯å®Œæˆ:")
        print(f"   - æ€»ç—…ä¾‹æ•°: {validation_results['total_cases']}")
        print(f"   - å­—æ®µè¦†ç›–ç‡: {len([f for f, c in validation_results['field_coverage'].items() if c > 0.5])}/{len(all_fields)}")
        print(f"   - ç©ºå­—æ®µæ•°: {len(validation_results['empty_fields'])}")
        
        return validation_results
    
    def run_full_pipeline(self, 
                         samples_per_group: int = 100, 
                         n_cases: int = 100,
                         validate: bool = True) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´çš„æ•°æ®ç”Ÿæˆæµæ°´çº¿"""
        print("ğŸš€ å¼€å§‹ç»“æ„åŒ–æ•°æ®ç”Ÿæˆæµæ°´çº¿...")
        
        # 1. ç”Ÿæˆå­—æ®µç»„æ ·æœ¬æ± 
        print("\nğŸ“Š æ­¥éª¤1: ç”Ÿæˆå­—æ®µç»„æ ·æœ¬æ± ")
        pools = self.generate_segment_pools(samples_per_group)
        
        # 2. ç”Ÿæˆç»„åˆç—…ä¾‹
        print("\nğŸ”— æ­¥éª¤2: ç”Ÿæˆç»„åˆç—…ä¾‹")
        cases = self.generate_combined_cases(n_cases)
        
        # 3. æ•°æ®éªŒè¯
        if validate:
            print("\nâœ… æ­¥éª¤3: æ•°æ®éªŒè¯")
            validation_results = self.validate_generated_data(cases)
        else:
            validation_results = {}
        
        print("\nğŸ‰ ç»“æ„åŒ–æ•°æ®ç”Ÿæˆå®Œæˆï¼")
        
        return {
            'pools': pools,
            'cases': cases,
            'validation': validation_results
        }

def main():
    """ä¸»å‡½æ•°"""
    generator = StructuredDataGenerator()
    results = generator.run_full_pipeline(
        samples_per_group=100,
        n_cases=100,
        validate=True
    )
    
    print(f"\nğŸ“ˆ ç”Ÿæˆç»Ÿè®¡:")
    print(f"   - å­—æ®µç»„æ ·æœ¬æ± : {len(results['pools'])} ä¸ª")
    print(f"   - ç»„åˆç—…ä¾‹: {len(results['cases'])} ä¸ª")
    print(f"   - è¾“å‡ºæ–‡ä»¶: data/combined_cases.json")

if __name__ == "__main__":
    main() 